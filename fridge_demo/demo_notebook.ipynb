{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">  \n",
    "Import packages\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import pickle \n",
    "from itertools import compress\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">  \n",
    "Check for cuda (optional?)\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.1+cu121'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()\n",
    "torchvision.__version__ \n",
    "torch.__version__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">  \n",
    "Import weights from best run\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\gener\\Desktop\\Refrigerator\\veggies2.jpg: 384x640 1 Bell pepper, 1 Cabbage, 1 Chili pepper, 1 Corn, 1 Garlic, 1 Radish, 1 Tomato, 42.4ms\n",
      "Speed: 8.1ms preprocess, 42.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mC:\\Users\\gener\\PycharmProjects\\pangolin\\runs\\detect\\predict7\u001b[0m\n",
      "1 label saved to C:\\Users\\gener\\PycharmProjects\\pangolin\\runs\\detect\\predict7\\labels\n"
     ]
    }
   ],
   "source": [
    "# 1) Import weights into YOLOv8\n",
    "# NOTE: change path to wherever your weights are stored\n",
    "trained = YOLO(\"C:/Users/gener/Desktop/Refrigerator/train6/weights/best.pt\")\n",
    "\n",
    "# 2) Run prediction on chosen test image\n",
    "test_image = \"veggies2.jpg\"\n",
    "results = trained(test_image, conf=0.01, save_txt=True)\n",
    "\n",
    "# 3) Print and save test image\n",
    "for i, r in enumerate(results):\n",
    "    # Plot results image\n",
    "    im_bgr = r.plot()  # BGR-order numpy array\n",
    "    im_rgb = Image.fromarray(im_bgr[..., ::-1])  # RGB-order PIL image\n",
    "    im_rgb.save(\"bound_box_result.jpg\")\n",
    "    # Show results to screen (in supported environments)\n",
    "    # r.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">  \n",
    "Convert result into usable string for Ingredient Lookup\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tomato': 1, 'Bell pepper': 1, 'Radish': 1, 'Corn': 1, 'Cabbage': 1, 'Garlic': 1, 'Chili pepper': 1}\n",
      "\n",
      "\n",
      "tomato,bell pepper,radish,corn,cabbage,garlic,chili pepper\n"
     ]
    }
   ],
   "source": [
    "# Extract text file info\n",
    "class_list = []\n",
    "\n",
    "# 1) Parse text file for bounding box labels\n",
    "# NOTE: Change directory to where .txt is stored\n",
    "with open('C:/Users/gener/PycharmProjects/pangolin/runs/detect/predict2/labels/veggies2.txt') as f:\n",
    "    lines = f.readlines() \n",
    "    for line in lines:\n",
    "        line = line.strip() \n",
    "        splitted_line = line.split()\n",
    "        class_list.append(splitted_line[0]) \n",
    "        \n",
    "results_dict = results[0].names\n",
    "\n",
    "# 2) Convert into ingredient counts dictionary\n",
    "ingredients_dict = {}\n",
    "for i in class_list:\n",
    "    index = int(i)\n",
    "    ingred_name = results_dict[index]\n",
    "    if ingred_name not in ingredients_dict:\n",
    "        ingredients_dict[ingred_name] = 0\n",
    "    ingredients_dict[ingred_name] += 1\n",
    "    \n",
    "print(ingredients_dict)\n",
    "\n",
    "# 3) Convert ingredient counts dictionary into comma-separated string\n",
    "print('\\n')\n",
    "food = ''\n",
    "first = True\n",
    "for ing in ingredients_dict.keys():\n",
    "    if not first:\n",
    "        food += ','\n",
    "    food += ing.lower()\n",
    "    first = False\n",
    "    j += 1\n",
    "print(food)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">\n",
    "    Functions for Ingredient Search\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = None\n",
    "df = None\n",
    "freq = {}\n",
    "\n",
    "def process_1K():\n",
    "    df = pd.read_csv('1K_dataset.csv')\n",
    "    df[\"IGS\"] = df[\"NER\"].apply(lambda x: set(x.lower().replace('[', '').replace(']', '').replace('\\\"', '').replace(', ', ',').split(\",\")))\n",
    "    d = df[\"IGS\"].values\n",
    "    np.save('1K_IGS', d)\n",
    "    return d\n",
    "\n",
    "def process_full():\n",
    "    df = pd.read_csv('full_dataset.csv')\n",
    "    df[\"IGS\"] = df[\"NER\"].apply(lambda x: set(x.lower().replace('[', '').replace(']', '').replace('\\\"', '').replace(', ', ',').split(\",\")))\n",
    "    d = df[\"IGS\"].values\n",
    "    np.save('full_IGS', d)\n",
    "    return d\n",
    "\n",
    "def process_optimized_1K():\n",
    "    df = pd.read_csv('1K_dataset.csv')\n",
    "    freq = {}\n",
    "    for i in range(df.shape[0]):\n",
    "        for ing in df[\"NER\"][i].lower().replace('[', '').replace(']', '').replace('\\\"', '').replace(', ', ',').split(\",\"):\n",
    "            if ing in freq: freq[ing].append(i)\n",
    "            else: freq[ing] = [i]\n",
    "        print(i)\n",
    "    with open('prcomputed_indices_1K.pkl', 'wb') as f:\n",
    "        pickle.dump(freq, f)\n",
    "    return df, freq\n",
    "\n",
    "def process_optimized_full():\n",
    "    df = pd.read_csv('full_dataset.csv')\n",
    "    freq = {}\n",
    "    for i in range(df.shape[0]):\n",
    "        for ing in df[\"NER\"][i].lower().replace('[', '').replace(']', '').replace('\\\"', '').replace(', ', ',').split(\",\"):\n",
    "            if ing in freq: freq[ing].append(i)\n",
    "            else: freq[ing] = [i]\n",
    "#         print(i)\n",
    "    with open('prcomputed_indices_full.pkl', 'wb') as f:\n",
    "        pickle.dump(freq, f)\n",
    "    return df, freq\n",
    "\n",
    "def initialize_1K():\n",
    "    df = pd.read_csv('1K_dataset.csv')\n",
    "    d = np.load('1k_IGS.npy', allow_pickle=True)\n",
    "    return df, d\n",
    "\n",
    "def initialize_full():\n",
    "    df = pd.read_csv('full_dataset.csv')\n",
    "    d = np.load('full_IGS.npy', allow_pickle=True)\n",
    "    return df, d\n",
    "\n",
    "def initialize_optimized_1K():\n",
    "    df = pd.read_csv('full_dataset.csv')\n",
    "    with open('prcomputed_indices_1K.pkl', 'rb') as f:\n",
    "        freq = pickle.load(f)\n",
    "    return df, freq\n",
    "\n",
    "def initialize_optimized_full():\n",
    "    df = pd.read_csv('full_dataset.csv')\n",
    "    with open('prcomputed_indices_full.pkl', 'rb') as f:\n",
    "        freq = pickle.load(f)\n",
    "    return df, freq\n",
    "\n",
    "\n",
    "def get_indices(ing_list):\n",
    "    ingredients = set(ing_list)\n",
    "    return np.where([ingredients.issubset(x) for x in d])\n",
    "\n",
    "def get_indices_optimized(ing_list):\n",
    "    if set(ing_list).issubset(freq.keys()):\n",
    "        base = freq[ing_list[0]]\n",
    "        for ing in ing_list:\n",
    "            base = np.intersect1d(base, freq[ing])\n",
    "        return base\n",
    "    else: return []\n",
    "\n",
    "def get_recipes(ing_list):\n",
    "    indices = get_indices(ing_list)\n",
    "    return df.loc[indices[0], [\"title\", \"link\"]]\n",
    "\n",
    "def get_recipes_optimized(ing_list):\n",
    "    indices = get_indices_optimized(ing_list)\n",
    "    return df.loc[indices, [\"title\", \"link\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">\n",
    "    Run Ingredient Search\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Can just run once (takes ~3 mins to run)\n",
    "process_optimized_full()\n",
    "print(\"process optimized done\")\n",
    "df, freq = initialize_optimized_full()\n",
    "print(\"initialization done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_recipes_optimized(food.split(\",\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
